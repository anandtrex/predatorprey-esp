\section{Related Work}
Task decomposition was tried in a soccer-keepaway task by Whiteson et. al. [3] to evolve an agent for playing keep away. They had separate networks for intercepting a pass, passing the ball to a team mate, evaluating if passing a ball to a particular team mate is viable, and moving to a good position for intercepting the ball. The agent was trained for each of these sub-tasks separately, and then all these networks were combined using decision tree in one case, and another network in the other case. Their performance was compared to the case where a single network was evolved for all four tasks simultaneously. In their study, they found that no matter what combination method was used, the task decomposition gave significantly better results than having a single monolithic network.  Apart from the overall performance, there was some interesting behaviour that was observed in the modular network that wasnâ€™t present in the monolithic network. In particular, the agent learnt to approach the ball from the direction opposite to that in which it was going to kick the ball.

There has also been some work done on learning tasks incrementally ([2] being one of them) -- starting with a simple task, and slowly increasing the task difficulty as the network learns. This approach is different from the task decomposition described here in that in the incremental learning approach, the task more or less remains the same, and just a few parameters of the task are varied to make the task more difficult. For instance, in the predator-prey domain, the speed of the prey is increased slowly to make the task more difficult incrementally. On the other hand, the task decomposition approach divides the task into specific sub-tasks and later combines them.
